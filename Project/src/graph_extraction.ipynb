{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for extracting graph from tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first section uses a dataset we extracted with no node features.\n",
    "We have extracted data in /data/drug_interactions.tsv with the following fields:\n",
    "\n",
    "- drug_interaction_id: id of drug A\n",
    "- name: name of drug A\n",
    "- description: interaction info of drug A with drug B\n",
    "- drugbank_id: id of drug B\n",
    "\n",
    "Now we want to extract a graph with nodes as the drugs and edges between each drug_interaction_id-drugbank_id pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = /home/jovyan/work/data/drug_interactions.tsv MapPartitionsRDD[283] at textFile at <console>:38\n",
       "header: String = drug_interaction_id\tname\tdescription\tdrugbank_id\n",
       "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[284] at filter at <console>:41\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read in data \n",
    "val lines = sc.textFile(\"/home/jovyan/work/data/drug_interactions.tsv\")\n",
    "// skip header\n",
    "val header = lines.first() // extract header\n",
    "val data = lines.filter(row => row != header) // filter out header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[String] = Array(DB06605\tApixaban\tApixaban may increase the anticoagulant activities of Lepirudin.\tDB00001, DB06695\tDabigatran etexilate\tDabigatran etexilate may increase the anticoagulant activities of Lepirudin.\tDB00001)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:27\n",
       "res1: Array[Array[String]] = Array(Array(DB06605, Apixaban, Apixaban may increase the anticoagulant activities of Lepirudin., DB00001), Array(DB06695, Dabigatran etexilate, Dabigatran etexilate may increase the anticoagulant activities of Lepirudin., DB00001), Array(DB01254, Dasatinib, The risk or severity of bleeding and hemorrhage can be increased when Dasatinib is combined with Lepirudin., DB00001), Array(DB01609, Deferasirox, The risk or severity of gastrointestinal bleeding can be increased when Lepirudin is combined with Deferasirox., DB00001))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = data.map(line => line.split(\"\\t\"))\n",
    "lines.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesDrugs: org.apache.spark.rdd.RDD[String] = /home/jovyan/work/data/drug_features.csv MapPartitionsRDD[5] at textFile at <console>:30\n",
       "header: Array[String] = Array(DB06605, Apixaban, Apixaban may increase the anticoagulant activities of Lepirudin., DB00001)\n",
       "data: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[6] at filter at <console>:33\n",
       "res2: Array[String] = Array(drug_id\ttype\tgroup\tinteractions, DB00001\tbiotech\t[approved]\t[DB06605, DB06695, DB01254, DB01609, DB01586, DB02123, DB02659, DB02691, DB03619, DB04348, DB05990, DB06777, DB08833, DB08834, DB08857, DB11622, DB11789, DB09075, DB09053, DB08935, DB06228, DB06206, DB09070, DB00932, DB00013, DB00163, DB09030, DB01381, DB01181, DB00468, DB00908, DB00675, DB00539, DB00806, DB00686, DB00583, DB00255, DB00269, DB00286, DB0...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read in all drugs (drug_interactions.tsv doesn't contain all drugs, only those with interactions) to get all drugs\n",
    "val linesDrugs = sc.textFile(\"/home/jovyan/work/data/drug_features.csv\")\n",
    "// skip header\n",
    "val header = lines.first() // extract header\n",
    "val data = lines.filter(row => row != header) // filter out header\n",
    "linesDrugs.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------------+\n",
      "|drug_id|   type|     group|        interactions|\n",
      "+-------+-------+----------+--------------------+\n",
      "|DB00001|biotech|[approved]|[DB06605, DB06695...|\n",
      "|DB00002|biotech|[approved]|[DB00012, DB00016...|\n",
      "|DB00003|biotech|[approved]|                null|\n",
      "+-------+-------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "all_drugs: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var all_drugs = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\"\\t\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/drug_features.csv\")\n",
    "all_drugs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinct_drugs: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [drug_id: string]\n",
       "res2: Long = 13580\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinct_drugs = all_drugs.select(\"drug_id\").distinct()\n",
    "distinct_drugs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugs: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at map at <console>:27\n",
       "res3: Array[String] = Array(DB00194, DB00741, DB00846, DB00912)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var drugs = distinct_drugs.select(\"drug_id\").rdd\n",
    "                    .map(x => x(0).toString) // prevent Array type\n",
    "drugs.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 13580\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce a drug -> node map containing the drug id and node id (which will be used as input to our graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drug2NodeMap: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[33] at zipWithIndex at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drug2NodeMap = drugs.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(String, Long)] = Array((DB00194,0), (DB00741,1), (DB00846,2), (DB00912,3), (DB01357,4))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug2NodeMap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Long = 13580\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug2NodeMap.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_drug_node_map: org.apache.spark.sql.DataFrame = [drug_id: string, node_id: bigint]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_drug_node_map = spark.createDataFrame(drug2NodeMap).toDF(\"drug_id\", \"node_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// save map for later use\n",
    "df_drug_node_map\n",
    "   .repartition(1)\n",
    "   .write.format(\"com.databricks.spark.csv\")\n",
    "   .option(\"header\", \"true\")\n",
    "   .save(\"/home/jovyan/work/data/drug2NodeMap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to map drugs in out drug interactions dataset to these node IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+-----------+\n",
      "|drug_interaction_id|                name|         description|drugbank_id|\n",
      "+-------------------+--------------------+--------------------+-----------+\n",
      "|            DB06605|            Apixaban|Apixaban may incr...|    DB00001|\n",
      "|            DB06695|Dabigatran etexilate|Dabigatran etexil...|    DB00001|\n",
      "|            DB01254|           Dasatinib|The risk or sever...|    DB00001|\n",
      "|            DB01609|         Deferasirox|The risk or sever...|    DB00001|\n",
      "|            DB01586|Ursodeoxycholic acid|The risk or sever...|    DB00001|\n",
      "+-------------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "interactions_df: org.apache.spark.sql.DataFrame = [drug_interaction_id: string, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var interactions_df = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\"\\t\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/drug_interactions.tsv\")\n",
    "interactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactions_rdd: org.apache.spark.rdd.RDD[(org.apache.spark.sql.Row, Long)] = ZippedWithIndexRDD[58] at zipWithIndex at <console>:26\n",
       "res9: Array[(org.apache.spark.sql.Row, Long)] = Array(([DB06605,Apixaban,Apixaban may increase the anticoagulant activities of Lepirudin.,DB00001],0))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var interactions_rdd = interactions_df.rdd.zipWithIndex()\n",
    "interactions_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactions_rdd2: org.apache.spark.rdd.RDD[(String, String, String, String, Long)] = MapPartitionsRDD[59] at map at <console>:26\n",
       "res10: Array[(String, String, String, String, Long)] = Array((DB06605,Apixaban,Apixaban may increase the anticoagulant activities of Lepirudin.,DB00001,0))\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interactions_rdd2 = interactions_rdd.map(x => (x._1(0).toString, x._1(1).toString, x._1(2).toString, x._1(3).toString, x._2))\n",
    "interactions_rdd2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+-----------+----------+\n",
      "|drug_interaction_id|                name|         description|drugbank_id|row_number|\n",
      "+-------------------+--------------------+--------------------+-----------+----------+\n",
      "|            DB06605|            Apixaban|Apixaban may incr...|    DB00001|         0|\n",
      "|            DB06695|Dabigatran etexilate|Dabigatran etexil...|    DB00001|         1|\n",
      "|            DB01254|           Dasatinib|The risk or sever...|    DB00001|         2|\n",
      "+-------------------+--------------------+--------------------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "interactions_df: org.apache.spark.sql.DataFrame = [drug_interaction_id: string, name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = spark.createDataFrame(interactions_rdd2).toDF(\"drug_interaction_id\", \"name\", \"description\", \"drugbank_id\", \"row_number\")\n",
    "interactions_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|drug_interaction_id|row_number|\n",
      "+-------------------+----------+\n",
      "|            DB06605|         0|\n",
      "|            DB06695|         1|\n",
      "+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "drugAs: org.apache.spark.sql.DataFrame = [drug_interaction_id: string, row_number: bigint]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drugAs = interactions_df.select(\"drug_interaction_id\", \"row_number\")\n",
    "drugAs.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 2668185\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugAs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugAWithNodeIDs: org.apache.spark.sql.DataFrame = [drug_interaction_id: string, row_number: bigint ... 2 more fields]\n",
       "res14: Long = 2668185\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var drugAWithNodeIDs = drugAs.join(df_drug_node_map, $\"drug_interaction_id\" === $\"drug_id\", \"left\")\n",
    "drugAWithNodeIDs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|drug_A_id|row_number|drug_A_node_id|\n",
      "+---------+----------+--------------+\n",
      "|  DB00194|    315697|             0|\n",
      "|  DB00741|      1855|             1|\n",
      "|  DB00741|     13266|             1|\n",
      "+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "drugAWithNodeIDs: org.apache.spark.sql.DataFrame = [drug_A_id: string, row_number: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugAWithNodeIDs = drugAWithNodeIDs.withColumnRenamed(\"drug_interaction_id\",\"drug_A_id\")\n",
    "           .withColumnRenamed(\"node_id\",\"drug_A_node_id\").drop(\"drug_id\")\n",
    "drugAWithNodeIDs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugBs: org.apache.spark.sql.DataFrame = [drugbank_id: string, row_number: bigint]\n",
       "drugBWithNodeIDs: org.apache.spark.sql.DataFrame = [drugbank_id: string, row_number: bigint ... 1 more field]\n",
       "res16: Long = 2668185\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drugBs = interactions_df.select(\"drugbank_id\", \"row_number\")\n",
    "var drugBWithNodeIDs = drugBs.join(df_drug_node_map, $\"drugbank_id\" === $\"drug_id\", \"left\")\n",
    "                        .withColumnRenamed(\"node_id\",\"drug_B_node_id\").drop(\"drug_id\")\n",
    "drugBWithNodeIDs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------+\n",
      "|drugbank_id|row_number|drug_B_node_id|\n",
      "+-----------+----------+--------------+\n",
      "|    DB00194|     77879|             0|\n",
      "+-----------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drugBWithNodeIDs.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edgesData: org.apache.spark.sql.DataFrame = [row_number: bigint, drug_A_id: string ... 3 more fields]\n",
       "res18: Long = 2668185\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join drugAWithNodeIDs and drugBWithNodeIDs to get the edges\n",
    "var edgesData = drugAWithNodeIDs.join(drugBWithNodeIDs, Seq(\"row_number\"), \"left\")\n",
    "edgesData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+--------------+\n",
      "|drug_A_id|drug_A_node_id|drugbank_id|drug_B_node_id|\n",
      "+---------+--------------+-----------+--------------+\n",
      "|  DB09030|         10315|    DB00001|          9529|\n",
      "|  DB00468|          4259|    DB00001|          9529|\n",
      "|  DB00056|            61|    DB00001|          9529|\n",
      "+---------+--------------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edgesData = edgesData.drop(\"row_number\")\n",
    "edgesData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "// save edges data for later use\n",
    "edgesData\n",
    "   .repartition(1)\n",
    "   .write.format(\"com.databricks.spark.csv\")\n",
    "   .option(\"header\", \"true\")\n",
    "   .save(\"/home/jovyan/work/data/edges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph extraction with Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use GCNs for link prediction, we need to extract some features for each node (drug). \n",
    "For this part, we use a dataset /data/drug_features.csv with the following fields:\n",
    "\n",
    "    - drug_id: id of drug \n",
    "    - type: name of drug A\n",
    "    - group: interaction info of drug A with drug B\n",
    "    - target_info: list of info extracted directly from xml, needed for extracting target gene name(s)\n",
    "    - enzyme_info: list of info extracted directly from xml, needed for extracting enzyme gene name(s)\n",
    "    - interactions: list of all drug ids this drug interacts with\n",
    "\n",
    "Now we want to extract a graph with nodes as the drugs containing features, and edges between each drug_interaction_id-drugbank_id pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+--------------------+\n",
      "|drug_id|          type|               group|        interactions|\n",
      "+-------+--------------+--------------------+--------------------+\n",
      "|DB00001|       biotech|          [approved]|[DB06605, DB06695...|\n",
      "|DB00002|       biotech|          [approved]|[DB00012, DB00016...|\n",
      "|DB00003|       biotech|          [approved]|                null|\n",
      "|DB00004|       biotech|[approved, invest...|[DB00012, DB00016...|\n",
      "|DB00005|       biotech|[approved, invest...|[DB01281, DB00026...|\n",
      "|DB00006|small molecule|[approved, invest...|[DB06605, DB06695...|\n",
      "|DB00007|small molecule|[approved, invest...|[DB09066, DB09083...|\n",
      "|DB00008|       biotech|[approved, invest...|[DB06643, DB00005...|\n",
      "+-------+--------------+--------------------+--------------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\"\\t\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/drug_features.csv\")\n",
    "df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.VectorIndexer\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. type feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "|          null|\n",
      "|       biotech|\n",
      "|small molecule|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// let's see what values we have here\n",
    "df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+--------------------+----------+-----------------+\n",
      "|drug_id|          type|               group|        interactions|is_biotech|is_small_molecule|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+\n",
      "|DB00001|       biotech|          [approved]|[DB06605, DB06695...|      true|            false|\n",
      "|DB00002|       biotech|          [approved]|[DB00012, DB00016...|      true|            false|\n",
      "|DB00003|       biotech|          [approved]|                null|      true|            false|\n",
      "|DB00004|       biotech|[approved, invest...|[DB00012, DB00016...|      true|            false|\n",
      "|DB00005|       biotech|[approved, invest...|[DB01281, DB00026...|      true|            false|\n",
      "|DB00006|small molecule|[approved, invest...|[DB06605, DB06695...|     false|             true|\n",
      "|DB00007|small molecule|[approved, invest...|[DB09066, DB09083...|     false|             true|\n",
      "|DB00008|       biotech|[approved, invest...|[DB06643, DB00005...|      true|            false|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 4 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// add is_biotech (0/1) and is_small_molecule columns\n",
    "df = df.withColumn(\"is_biotech\", col(\"type\") === \"biotech\")\n",
    "df = df.withColumn(\"is_small_molecule\", col(\"type\") === \"small molecule\")\n",
    "df.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. group feature\n",
    "This feature is a list of values from: ['withdrawn', 'illicit', 'vet_approved', 'investigational', 'approved', 'experimental', 'nutraceutical'] (extracted using pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "|drug_id|          type|               group|        interactions|is_biotech|is_small_molecule|withdrawn|illicit|vet_approved|investigational|approved|experimental|nutraceutical|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "|DB00001|       biotech|          [approved]|[DB06605, DB06695...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00002|       biotech|          [approved]|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00003|       biotech|          [approved]|                null|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00004|       biotech|[approved, invest...|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00005|       biotech|[approved, invest...|[DB01281, DB00026...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00006|small molecule|[approved, invest...|[DB06605, DB06695...|     false|             true|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00007|small molecule|[approved, invest...|[DB09066, DB09083...|     false|             true|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00008|       biotech|[approved, invest...|[DB06643, DB00005...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"withdrawn\", col(\"group\").contains(\"withdrawn\"))\n",
    "df = df.withColumn(\"illicit\", col(\"group\").contains(\"illicit\"))\n",
    "df = df.withColumn(\"vet_approved\", col(\"group\").contains(\"vet_approved\"))\n",
    "df = df.withColumn(\"investigational\", col(\"group\").contains(\"investigational'\"))\n",
    "df = df.withColumn(\"approved\", col(\"group\").contains(\"approved\"))\n",
    "df = df.withColumn(\"experimental\", col(\"group\").contains(\"experimental\"))\n",
    "df = df.withColumn(\"nutraceutical\", col(\"group\").contains(\"nutraceutical\"))\n",
    "df.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Nodes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "|drug_id|          type|               group|        interactions|is_biotech|is_small_molecule|withdrawn|illicit|vet_approved|investigational|approved|experimental|nutraceutical|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "|DB00001|       biotech|          [approved]|[DB06605, DB06695...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00002|       biotech|          [approved]|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00003|       biotech|          [approved]|                null|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00004|       biotech|[approved, invest...|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00005|       biotech|[approved, invest...|[DB01281, DB00026...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00006|small molecule|[approved, invest...|[DB06605, DB06695...|     false|             true|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00007|small molecule|[approved, invest...|[DB09066, DB09083...|     false|             true|    false|  false|       false|          false|    true|       false|        false|\n",
      "|DB00008|       biotech|[approved, invest...|[DB06643, DB00005...|      true|            false|    false|  false|       false|          false|    true|       false|        false|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val df2 = df.withColumn(\"interactions\", explode(array(col(\"interactions\"))))\n",
    "// df2.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|drug_id|node_id|\n",
      "+-------+-------+\n",
      "|DB00194|      0|\n",
      "|DB00741|      1|\n",
      "|DB00846|      2|\n",
      "|DB00912|      3|\n",
      "|DB01357|      4|\n",
      "|DB01460|      5|\n",
      "|DB01979|      6|\n",
      "+-------+-------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_drug_node_map: org.apache.spark.sql.DataFrame = [drug_id: string, node_id: string]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_drug_node_map = spark.read.options(Map(\"delimiter\"->\",\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/drug2NodeMap.csv\")\n",
    "df_drug_node_map.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Long = 13608\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "|drug_id|   type|               group|        interactions|is_biotech|is_small_molecule|withdrawn|illicit|vet_approved|investigational|approved|experimental|nutraceutical|node_id|\n",
      "+-------+-------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "|DB00001|biotech|          [approved]|[DB06605, DB06695...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   9529|\n",
      "|DB00002|biotech|          [approved]|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   3685|\n",
      "|DB00003|biotech|          [approved]|                null|      true|            false|    false|  false|       false|          false|    true|       false|        false|   2938|\n",
      "|DB00004|biotech|[approved, invest...|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   6476|\n",
      "|DB00005|biotech|[approved, invest...|[DB01281, DB00026...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   1028|\n",
      "+-------+-------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "node_features: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val node_features = df.join(df_drug_node_map, Seq(\"drug_id\"), \"left\")\n",
    "node_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Long = 13608\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features\n",
    "   .repartition(1)\n",
    "   .write.format(\"com.databricks.spark.csv\")\n",
    "   .option(\"header\", \"true\")\n",
    "   .save(\"/home/jovyan/work/data/node_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and Enzyme gene names\n",
    "Now we will join our node features with our dataset containing gene names for target and enzyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "|drug_id|          type|               group|        interactions|is_biotech|is_small_molecule|withdrawn|illicit|vet_approved|investigational|approved|experimental|nutraceutical|node_id|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "|DB00001|       biotech|          [approved]|[DB06605, DB06695...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   3022|\n",
      "|DB00002|       biotech|          [approved]|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   1436|\n",
      "|DB00003|       biotech|          [approved]|                null|      true|            false|    false|  false|       false|          false|    true|       false|        false|   null|\n",
      "|DB00004|       biotech|[approved, invest...|[DB00012, DB00016...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   1625|\n",
      "|DB00005|       biotech|[approved, invest...|[DB01281, DB00026...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   3623|\n",
      "|DB00006|small molecule|[approved, invest...|[DB06605, DB06695...|     false|             true|    false|  false|       false|          false|    true|       false|        false|   1503|\n",
      "|DB00007|small molecule|[approved, invest...|[DB09066, DB09083...|     false|             true|    false|  false|       false|          false|    true|       false|        false|    236|\n",
      "|DB00008|       biotech|[approved, invest...|[DB06643, DB00005...|      true|            false|    false|  false|       false|          false|    true|       false|        false|   3052|\n",
      "+-------+--------------+--------------------+--------------------+----------+-----------------+---------+-------+------------+---------------+--------+------------+-------------+-------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "node_features: org.apache.spark.sql.DataFrame = [drug_id: string, type: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var node_features = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/node_features.csv\")\n",
    "node_features.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+\n",
      "|drugbank_id|        target-gname|enzyme-gname|\n",
      "+-----------+--------------------+------------+\n",
      "|    DB00001|                  F2|        null|\n",
      "|    DB00002|EGFR,FCGR3B,C1QA,...|        null|\n",
      "|    DB00003|                null|        null|\n",
      "|    DB00004|   IL2RA,IL2RB,IL2RG|        null|\n",
      "|    DB00005|TNF,FCGR1A,FCGR2A...|        null|\n",
      "|    DB00006|                  F2|         MPO|\n",
      "|    DB00007|               GNRHR|        null|\n",
      "|    DB00008|       IFNAR2,IFNAR1|      CYP1A2|\n",
      "+-----------+--------------------+------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_gname: org.apache.spark.sql.DataFrame = [drugbank_id: string, target-gname: string ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df_gname = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\"\\t\", \"header\"->\"true\"))\n",
    "  .csv(\"/home/jovyan/work/data/drug_gname.tsv\")\n",
    "df_gname.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target-gname and enzyme-gname are a (possible) list of gene names, we will create one field per gene name which contains a 0 or 1 value per drug row depending if it appears for target, and then same for enzymes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+\n",
      "|drugbank_id|        target-gname|enzyme-gname|\n",
      "+-----------+--------------------+------------+\n",
      "|    DB00001|                [F2]|        null|\n",
      "|    DB00002|[EGFR, FCGR3B, C1...|        null|\n",
      "|    DB00003|                null|        null|\n",
      "|    DB00004|[IL2RA, IL2RB, IL...|        null|\n",
      "|    DB00005|[TNF, FCGR1A, FCG...|        null|\n",
      "|    DB00006|                [F2]|       [MPO]|\n",
      "|    DB00007|             [GNRHR]|        null|\n",
      "|    DB00008|    [IFNAR2, IFNAR1]|    [CYP1A2]|\n",
      "|    DB00009|[PLG, FGA, PLAUR,...|        null|\n",
      "|    DB00010|             [GHRHR]|        null|\n",
      "+-----------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_gname: org.apache.spark.sql.DataFrame = [drugbank_id: string, target-gname: array<string> ... 1 more field]\n",
       "df_gname: org.apache.spark.sql.DataFrame = [drugbank_id: string, target-gname: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gname = df_gname.withColumn(\"target-gname\", split($\"target-gname\", \",\"))\n",
    "df_gname = df_gname.withColumn(\"enzyme-gname\", split($\"enzyme-gname\", \",\"))\n",
    "df_gname.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[416] at map at <console>:46\n"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd  = df.select(\"interactions\").rdd.map(r => r(0))\n",
    "// var df3 = spark.createDataFrame(rdd).toDF()\n",
    "// df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+\n",
      "|drug_id|   type|               group|        interactions|\n",
      "+-------+-------+--------------------+--------------------+\n",
      "|DB00001|biotech|          [approved]|[DB06605, DB06695...|\n",
      "|DB00002|biotech|          [approved]|[DB00012, DB00016...|\n",
      "|DB00003|biotech|          [approved]|                null|\n",
      "|DB00004|biotech|[approved, invest...|[DB00012, DB00016...|\n",
      "|DB00005|biotech|[approved, invest...|[DB01281, DB00026...|\n",
      "+-------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "44: error: value split is not a member of org.apache.spark.sql.types.StructField",
     "output_type": "error",
     "traceback": [
      "<console>:44: error: value split is not a member of org.apache.spark.sql.types.StructField",
      "       val r = udf((input: StructField) => input.split(\",\").map(_.toString))",
      "                                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val r = udf((input: StructField) => input.split(\",\").map(_.toString))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "50: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:50: error: type mismatch;",
      " found   : String(\"interactions\")",
      " required: org.apache.spark.sql.Column",
      "       df.withColumn(\"test\", r(\"interactions\")).show(5)",
      "                               ^",
      ""
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test\", r(\"interactions\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+\n",
      "|drug_id|   type|               group|        interactions|                test|\n",
      "+-------+-------+--------------------+--------------------+--------------------+\n",
      "|DB00001|biotech|          [approved]|[DB06605, DB06695...|[[DB06605, DB0669...|\n",
      "|DB00002|biotech|          [approved]|[DB00012, DB00016...|[[DB00012, DB0001...|\n",
      "|DB00003|biotech|          [approved]|                null|                null|\n",
      "|DB00004|biotech|[approved, invest...|[DB00012, DB00016...|[[DB00012, DB0001...|\n",
      "|DB00005|biotech|[approved, invest...|[DB01281, DB00026...|[[DB01281, DB0002...|\n",
      "+-------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "        \"test\",\n",
    "        split(col(\"interactions\"), \",\\\\*\").cast(\"Array<String>\").alias(\"ev\")\n",
    " ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- drug_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- interactions: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"interactions\", array_to_string_udf(df[\"interactions_as_arr\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res165: Array[org.apache.spark.sql.Row] = Array([[DB06605, DB06695, DB01254, DB01609, DB01586, DB02123, DB02659, DB02691, DB03619, DB04348, DB05990, DB06777, DB08833, DB08834, DB08857, DB11622, DB11789, DB09075, DB09053, DB08935, DB06228, DB06206, DB09070, DB00932, DB00013, DB00163, DB09030, DB01381, DB01181, DB00468, DB00908, DB00675, DB00539, DB00806, DB00686, DB00583, DB00255, DB00269, DB00286, DB00783, DB00977, DB01357, DB04573, DB04574, DB04575, DB07931, DB09317, DB09318, DB09369, DB09381, DB11478, DB11674, DB12487, DB13143, DB13386, DB13418, DB13952, DB13953, DB13954, DB13956, DB15334, DB15335, DB09211, DB00159, DB00244, DB00328, DB00461, DB00465, DB00469, DB00482, DB00500, DB00533, DB00554, DB00573, DB00580, DB00586, DB00605, DB00712, DB00749, DB00784, DB00788, DB00795, DB00812, ...\n"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"interactions\").rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "45: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:45: error: type mismatch;",
      " found   : List[org.apache.spark.sql.Column]",
      " required: org.apache.spark.sql.Column",
      "       df.withColumn(\"list_interactions_id\", List(col(\"interactions\"))).show(2)",
      "                                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "df.withColumn(\"list_interactions_id\", List(col(\"interactions\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- drug_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- interactions: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[448] at rdd at <console>:46\n"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd = df.select(\"interactions\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t: org.apache.spark.rdd.RDD[List[org.apache.spark.sql.Row]] = MapPartitionsRDD[454] at map at <console>:46\n"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var t = rdd.map(x => List(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index: org.apache.spark.rdd.RDD[(List[org.apache.spark.sql.Row], Long)] = ZippedWithIndexRDD[463] at zipWithIndex at <console>:46\n"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var index = t.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res204: Long = 13608\n"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myIndex: Int = 5\n",
       "values: org.apache.spark.rdd.RDD[(List[org.apache.spark.sql.Row], Long)] = MapPartitionsRDD[462] at filter at <console>:50\n"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var myIndex = 5\n",
    "var values = (t.zipWithIndex()\n",
    "            .filter(_._2==myIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res198: Array[(List[org.apache.spark.sql.Row], Long)] = Array((List([[DB06605, DB06695, DB01254, DB01609, DB01586, DB02123, DB02659, DB02691, DB03619, DB04348, DB05990, DB06777, DB08833, DB08834, DB08857, DB11622, DB11789, DB09075, DB09053, DB08935, DB06228, DB06206, DB09070, DB00932, DB00013, DB00163, DB09030, DB01381, DB01181, DB00468, DB00908, DB00675, DB00539, DB00806, DB00686, DB00583, DB00255, DB00269, DB00286, DB00783, DB00977, DB01357, DB04573, DB04574, DB04575, DB07931, DB09317, DB09318, DB09369, DB09381, DB11478, DB11674, DB12487, DB13143, DB13386, DB13418, DB13952, DB13953, DB13954, DB13956, DB15334, DB15335, DB09211, DB00159, DB00244, DB00328, DB00461, DB00465, DB00469, DB00482, DB00500, DB00533, DB00554, DB00573, DB00580, DB00586, DB00605, DB00712, DB00749, DB00784, DB00788...\n"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// experiment code for one hot encoding/categorising features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|category1|categoryIndex2|\n",
      "+---------+--------------+\n",
      "|        a|           1.0|\n",
      "|        b|           0.0|\n",
      "|        a|           1.0|\n",
      "|        a|           2.0|\n",
      "|        c|           1.0|\n",
      "|        d|           0.0|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category1: string, categoryIndex2: double]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.createDataFrame(Seq(\n",
    "  (\"a\", 1.0),\n",
    "  (\"b\", 0.0),\n",
    "  (\"a\", 1.0),\n",
    "  (\"a\", 2.0),\n",
    "  (\"c\", 1.0),\n",
    "  (\"d\", 0.0)\n",
    ")).toDF(\"category1\", \"categoryIndex2\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n",
      "|category1|categoryIndex2|categoryIndex1|\n",
      "+---------+--------------+--------------+\n",
      "|        a|           1.0|           0.0|\n",
      "|        b|           0.0|           1.0|\n",
      "|        a|           1.0|           0.0|\n",
      "|        a|           2.0|           0.0|\n",
      "|        c|           1.0|           2.0|\n",
      "|        d|           0.0|           3.0|\n",
      "+---------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexedDf: org.apache.spark.sql.DataFrame = [category1: string, categoryIndex2: double ... 1 more field]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// need to convert string fields to integer before can one hot encode\n",
    "val indexedDf = new StringIndexer().setInputCol(\"category1\").setOutputCol(\"categoryIndex1\").fit(df).transform(df);\n",
    "indexedDf.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+-------------+\n",
      "|category1|categoryIndex2|categoryIndex1| categoryVec1|\n",
      "+---------+--------------+--------------+-------------+\n",
      "|        a|           1.0|           0.0|(4,[0],[1.0])|\n",
      "|        b|           0.0|           1.0|(4,[1],[1.0])|\n",
      "|        a|           1.0|           0.0|(4,[0],[1.0])|\n",
      "|        a|           2.0|           0.0|(4,[0],[1.0])|\n",
      "|        c|           1.0|           2.0|(4,[2],[1.0])|\n",
      "|        d|           0.0|           3.0|(4,[3],[1.0])|\n",
      "+---------+--------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_b4f946501dc4\n",
       "model: org.apache.spark.ml.feature.OneHotEncoderModel = OneHotEncoderModel: uid=oneHotEncoder_b4f946501dc4, dropLast=false, handleInvalid=error, numInputCols=1, numOutputCols=1\n",
       "encoded: org.apache.spark.sql.DataFrame = [category1: string, categoryIndex2: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoder()\n",
    "    .setDropLast(false) // don't dtop last column (spark does this for sparseness)\n",
    "     .setInputCols(Array(\"categoryIndex1\"))\n",
    "    .setOutputCols(Array(\"categoryVec1\"))\n",
    "\n",
    "val model = encoder.fit(indexedDf)\n",
    "val encoded = model.transform(indexedDf)\n",
    "encoded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// now we want to figure out how to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+----+\n",
      "|country|points|contestant|year|\n",
      "+-------+------+----------+----+\n",
      "|  India|    30|         2|1991|\n",
      "|     Us|    30|         1|1987|\n",
      "|  India|    30|         2|1992|\n",
      "|  China|    30|         3|1993|\n",
      "|  India|    30|         2|1980|\n",
      "|     Us|    30|         1|1990|\n",
      "|  India|    30|         2|1982|\n",
      "|  China|    30|         3|1994|\n",
      "+-------+------+----------+----+\n",
      "\n",
      "+-------+------+----------+----+-------------+\n",
      "|country|points|contestant|year|country_index|\n",
      "+-------+------+----------+----+-------------+\n",
      "|  India|    30|         2|1991|          0.0|\n",
      "|     Us|    30|         1|1987|          2.0|\n",
      "|  India|    30|         2|1992|          0.0|\n",
      "|  China|    30|         3|1993|          1.0|\n",
      "|  India|    30|         2|1980|          0.0|\n",
      "|     Us|    30|         1|1990|          2.0|\n",
      "|  India|    30|         2|1982|          0.0|\n",
      "|  China|    30|         3|1994|          1.0|\n",
      "+-------+------+----------+----+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleData: Seq[(String, Int, Int, Int)] = List((India,30,2,1991), (Us,30,1,1987), (India,30,2,1992), (China,30,3,1993), (India,30,2,1980), (Us,30,1,1990), (India,30,2,1982), (China,30,3,1994))\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2e2996fe\n",
       "sampleDataDf: org.apache.spark.sql.DataFrame = [country: string, points: int ... 2 more fields]\n",
       "sampleIndexedDf: org.apache.spark.sql.DataFrame = [country: string, points: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// how to categorise strings\n",
    "var sampleData = Seq(\n",
    "(\"India\", 30, 2, 1991),\n",
    "(\"Us\", 30, 1, 1987),\n",
    "(\"India\", 30, 2, 1992),\n",
    "(\"China\", 30, 3, 1993),\n",
    "(\"India\", 30, 2, 1980),\n",
    "(\"Us\", 30, 1, 1990),\n",
    "(\"India\", 30, 2, 1982),\n",
    "(\"China\", 30, 3, 1994)\n",
    ");\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"test\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate();\n",
    "\n",
    "val sampleDataDf = spark.createDataFrame(sampleData).toDF(\"country\", \"points\", \"contestant\", \"year\");\n",
    "val sampleIndexedDf = new StringIndexer().setInputCol(\"country\").setOutputCol(\"country_index\").fit(sampleDataDf).transform(sampleDataDf);\n",
    "sampleDataDf.show();\n",
    "sampleIndexedDf.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleData2: Seq[(String, Int)] = List((Us,15), (India,3), (China,7))\n",
       "sampleData2Df: org.apache.spark.sql.DataFrame = [country_name: string, node_id: int]\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var sampleData2 = Seq(\n",
    "(\"Us\", 15),\n",
    "(\"India\", 3),\n",
    "(\"China\", 7)\n",
    ");\n",
    "val sampleData2Df = spark.createDataFrame(sampleData2).toDF(\"country_name\", \"node_id\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|country_name|node_id|\n",
      "+------------+-------+\n",
      "|          Us|     15|\n",
      "|       India|      3|\n",
      "|       China|      7|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleData2Df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+----+------------+-------+\n",
      "|country|points|contestant|year|country_name|node_id|\n",
      "+-------+------+----------+----+------------+-------+\n",
      "|     Us|    30|         1|1990|          Us|     15|\n",
      "|     Us|    30|         1|1987|          Us|     15|\n",
      "|  India|    30|         2|1982|       India|      3|\n",
      "|  India|    30|         2|1980|       India|      3|\n",
      "|  India|    30|         2|1992|       India|      3|\n",
      "|  India|    30|         2|1991|       India|      3|\n",
      "|  China|    30|         3|1994|       China|      7|\n",
      "|  China|    30|         3|1993|       China|      7|\n",
      "+-------+------+----------+----+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join_df: org.apache.spark.sql.DataFrame = [country: string, points: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val join_df = sampleDataDf.join(sampleData2Df, $\"country\" === $\"country_name\", \"right\")\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|     region|           countries|\n",
      "+-----------+--------------------+\n",
      "|Scandinavia|[Norway, Denmark,...|\n",
      "| South Asia|[India, Pakistan,...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleData: Seq[(String, List[String])] = List((Scandinavia,List(Norway, Denmark, Sweden)), (South Asia,List(India, Pakistan, Bangladesh)))\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2e089964\n",
       "sampleDataDf: org.apache.spark.sql.DataFrame = [region: string, countries: array<string>]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define sample data for one hot encode\n",
    "var sampleData = Seq(\n",
    "(\"Scandinavia\", List(\"Norway\", \"Denmark\", \"Sweden\")),\n",
    "(\"South Asia\", List(\"India\", \"Pakistan\", \"Bangladesh\")),\n",
    ");\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"test\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate();\n",
    "\n",
    "val sampleDataDf = spark.createDataFrame(sampleData).toDF(\"region\", \"countries\");\n",
    "sampleDataDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " requirement failed: Column countries must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually array<string>.",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: requirement failed: Column countries must be of type struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually array<string>.",
      "  at scala.Predef$.require(Predef.scala:281)",
      "  at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:45)",
      "  at org.apache.spark.ml.feature.VectorIndexer.transformSchema(VectorIndexer.scala:163)",
      "  at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)",
      "  at org.apache.spark.ml.feature.VectorIndexer.fit(VectorIndexer.scala:142)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "// convert fields to categorical\n",
    "val sampleIndexedDf = new VectorIndexer().setInputCol(\"countries\").setOutputCol(\"countries_categorical\").fit(sampleDataDf).transform(sampleDataDf);\n",
    "sampleDataDf.show();\n",
    "sampleIndexedDf.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
